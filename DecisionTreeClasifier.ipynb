{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqkwtbE4ZdgEA32P0Aax6x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jrk373/MachineLearningDemo/blob/main/DecisionTreeClasifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Artificial Intelligence Using Decision Tree Classifiers\n",
        "\n",
        "John Ryan Kivela, MA\n",
        "\n",
        "Ventura, CA"
      ],
      "metadata": {
        "id": "cbsa-zCUKlb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This is a walkthrough demonstration of fundemental Machine Learning concepts and techniques used in developing Artificial Intelligence (AI).\n",
        "\n",
        "The activity will create a Narrow Artificial Intelligence, called Clepy 1.0, who will tell us if an employee is likely to leave the company.\n",
        "\n",
        "Narrow AI, also known as Weak AI, is a type of artificial intelligence designed to perform a specific task or solve a particular problem with high efficiency. Unlike General AI, Narrow AI is limited in scope and cannot adapt to tasks outside its predefined domain.\n",
        "\n",
        "**Meet Clepy!**\n",
        "\n",
        "Clepy is the misunderstood Gen Z granddaughter of fan favorite, Clippy. Clepy's domain is the ability to predict attrition based on employee characteristics.\n",
        "\n",
        "She delivers answers, with attitude.\n",
        "\n",
        "<img src=\"https://github.com/Jrk373/MachineLearningDemo/blob/main/narbgirlwip3.jpeg?raw=true\" alt=\"Clepy\" width=\"300\" height=\"200\" />\n",
        "\n",
        "## Intended Audience\n",
        "\n",
        "- The intended audience is a discerning group of professionals with strong data literacy, advanced education in mathematics like Central Limits Theorum (mean, median, mode, standard deviation, etc.), and basic Linear Algebra.\n",
        "\n",
        "- It is not necessary to understand computer languages for this activity. This notebook primarily speaks in Python, but it is programmed to run all code and calculations out-of-the-box for any participant.\n",
        "\n",
        "## Participant Objectives\n",
        "\n",
        "- Understand the basic concept of Decision Tree Classifer models.\n",
        "- Develop understanding of programming a decision tree with Python.\n",
        "- Build a decision tree model algorythm for Clepy.\n",
        "- Design and deploy a web application for Clepy's model.\n",
        "\n",
        "## Enjoy!\n",
        "\n"
      ],
      "metadata": {
        "id": "WCODS2YdKOeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t"
      ],
      "metadata": {
        "id": "76ybhgoG3s8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method\n",
        "\n",
        "## Materials\n",
        "\n",
        "- This open source Python Notebook.\n",
        "- Materials are located in a public Github repository.\n",
        "\n",
        "  https://github.com/Jrk373/MachineLearningDemo\n",
        "\n",
        "- This course requires participants to have a Google Account.\n",
        "\n",
        "## CRISP-DM\n",
        "\n",
        "CRISP-DM stands for Cross-Industry Standard Process for Data Mining. It's a popular method used to guide data mining and data science projects.\n",
        "\n",
        "<img src=\"https://github.com/Jrk373/MachineLearningDemo/blob/main/crisp_process.jpg?raw=true\" alt=\"Clepy\" width=\"400\" height=\"400\" />\n",
        "\n",
        "\n",
        "\n",
        "The process is divided into six main phases:\n",
        "\n",
        "- **Business Understanding:** Understand the project's goals and requirements from a business perspective.\n",
        "- **Data Understanding:** Collect and analyze the data to understand its characteristics.\n",
        "- **Data Preparation:** Clean and prepare the data for analysis.\n",
        "- **Modeling:** Apply different modeling techniques to the prepared data.\n",
        "- **Evaluation:** Assess the models to ensure they meet the business objectives.\n",
        "- **Deployment:** Implement the model in the real-world environment and monitor its performance.\n",
        "\n",
        "## Object Oriented Programming\n",
        "\n",
        "Object-Oriented Programming is a way of organizing code using objects that represent real-world things with data and methods.\n",
        "\n",
        "The three key features of Object-Oriented Programming are **Encapsulation**, **Inheritance**, and **Polymorphism**. Below is a description of each feature along with corresponding Python code snippets.\n",
        "\n",
        "## Encapsulation\n",
        "\n",
        "Encapsulation is the bundling of data (attributes) and methods (functions) that operate on the data into a single unit or class.\n",
        "\n",
        "Example Code:\n",
        "\n",
        "```python\n",
        "account = BankAccount()\n",
        "account.deposit(100)\n",
        "print(account.get_balance())  # Output: 100\n",
        "```\n",
        "\n",
        "## Inheritance\n",
        "\n",
        "Inheritance allows a class to inherit attributes and methods from another class, promoting code reusability and establishing a relationship between classes.\n",
        "\n",
        "Example Code:\n",
        "\n",
        "```python\n",
        "dog = Dog()\n",
        "print(dog.speak())   # Output: Bark\n",
        "print(isinstance(dog, Animal))  # Output: True (Dog is an Animal)\n",
        "```\n",
        "\n",
        "## Polymorphism\n",
        "\n",
        "Polymorphism allows the same methods to do different things based on the object it is acting upon.\n",
        "\n",
        "Example Code:\n",
        "\n",
        "```python\n",
        "dog = Dog()\n",
        "cat = Cat()\n",
        "\n",
        "animal_sound(dog)   # Output: Bark\n",
        "animal_sound(cat)   # Output: Meow\n",
        "```\n",
        "\n",
        "## Python\n",
        "\n",
        "Python is an object oreiented programming language known for its simplicity and readability. It is widely used for web development, data analysis, machine learning, automation, scientific computing, and more. Python's philosophy emphasizes easy-to-understand syntax that allows developers to express concepts with brevity and explain to business teams.\n",
        "\n",
        "## Jupyter Notebooks\n",
        "\n",
        "Jupyter Notebooks are an open-source, interactive computing environment that allows users to create and share documents containing live code, equations, visualizations, and narrative text. They are widely used in data science, machine learning, academic research, and scientific computing due to their versatility and ease of use.\n",
        "\n",
        "## Resources\n",
        "\n",
        "The notebook is inspired by the ODSC West 2024 AI Bootcamp and Statquest Classification Trees in Python. This notebook partners with AI like ChatGPT and Gemini as generators of code and content.\n",
        "\n"
      ],
      "metadata": {
        "id": "YRKxfbIec4DZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Procedure\n",
        "\n",
        "The instructor will walk through the Notebook with the audience watching and discussing. Then the instructor and the class go through the notebook together. The learner can then go on to use the Notebook on their own."
      ],
      "metadata": {
        "id": "YSqmRo8xdPUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 1: Business Understanding"
      ],
      "metadata": {
        "id": "BUmuAFt1KjDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Business Understanding phase of CRISP-DM focuses on defining the project’s goals and objectives from a business perspective. This stage ensures that the data science work aligns with the organization’s needs and delivers value.\n",
        "\n",
        "This is the most important step to building a product that will meet the business needs of the company.\n",
        "\n",
        "Ironically, we will spend very little time on it for this presentation."
      ],
      "metadata": {
        "id": "Cj6uBP4AXgie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 2: Data Understanding"
      ],
      "metadata": {
        "id": "8WJYcLmbKmRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Data Understanding phase of the CRISP-DM (Cross-Industry Standard Process for Data Mining) framework focuses on exploring and analyzing the available data to ensure it is suitable for the project's goals.\n",
        "\n",
        "It involves the following steps:\n",
        "\n",
        "- **Data Collection:** Gather initial data from relevant sources.\n",
        "\n",
        "- **Data Description:** Summarize key attributes, including data types, formats, and basic statistics (e.g., means, counts, ranges).\n",
        "\n",
        "- **Data Exploration:** Use visualizations and analyses to identify patterns, trends, or potential relationships in the data.\n",
        "\n",
        "- **Data Quality Assessment:** Check for issues such as missing values, outliers, inconsistencies, or inaccuracies.\n",
        "The objective is to develop insights into the data, identify challenges, and determine whether it can support the project's objectives effectively."
      ],
      "metadata": {
        "id": "PjakemwlXs_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Set\n",
        "\n",
        "The **Employee Attrition for Healthcare Dataset** is a dataset used to analyze and predict employee turnover (attrition) within healthcare organizations. Attrition in this context indicates the rate at which employees leave their jobs, whether voluntarily (resignation, retirement) or involuntarily (termination).\n",
        "\n",
        "https://www.kaggle.com/datasets/jpmiller/employee-attrition-for-healthcare\n",
        "\n",
        "Understanding and managing employee attrition is crucial in the healthcare sector, where staff shortages can directly impact patient care."
      ],
      "metadata": {
        "id": "f80n-AmWKqmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import pandas as pd  # Importing pandas for handling dataframes\n",
        "\n",
        "# Pathway to Github repository\n",
        "url = 'https://raw.githubusercontent.com/Jrk373/MachineLearningDemo/main/watson_healthcare_modified.csv'\n",
        "file_path = 'watson_healthcare_modified.csv'\n",
        "\n",
        "# Retrieve the file\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print('Successfully downloaded', file_path)\n",
        "    print('Data successfully loaded as data frame \"df\"')\n",
        "\n",
        "# Mop up some errors in the data set\n",
        "except pd.errors.ParserError as e:\n",
        "    print(\"ParserError encountered:\", e)\n",
        "    print(\"Attempting to load with alternative options...\")\n",
        "    # Suppress warnings for error_bad_lines (deprecated since pandas v1.3.0)\n",
        "    df = pd.read_csv(file_path, delimiter=',', on_bad_lines='skip')\n",
        "    print(\"Data loaded with error handling.\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZJAMAAG7pHgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Dictionary\n",
        "\n",
        "**Employee Attrition for Healthcare Dataset - Data Dictionary**\n",
        "\n",
        "https://github.com/Jrk373/MachineLearningDemo/blob/main/watson_healthcare_modified.csv\n",
        "\n",
        "This document provides a detailed data dictionary for the \"Employee Attrition for Healthcare\" dataset available on Kaggle.\n",
        "\n",
        "| **Column Name**             | **Data Type** | **Description**                                                                 |\n",
        "|-----------------------------|---------------|---------------------------------------------------------------------------------|\n",
        "| **Age**                     | Integer       | Age of the employee.                                                           |\n",
        "| **Attrition**               | String        | Whether the employee has left the company ('Yes' or 'No').                     |\n",
        "| **BusinessTravel**          | String        | Frequency of business travel ('Non-Travel', 'Travel_Rarely', 'Travel_Frequently'). |\n",
        "| **DailyRate**               | Integer       | Daily rate of the employee.                                                    |\n",
        "| **Department**              | String        | Department where the employee works.                                           |\n",
        "| **DistanceFromHome**        | Integer       | Distance from home to workplace (in miles).                                    |\n",
        "| **Education**               | Integer       | Education level (1: 'Below College', 2: 'College', 3: 'Bachelor', 4: 'Master', 5: 'Doctor'). |\n",
        "| **EducationField**          | String        | Field of education.                                                            |\n",
        "| **EmployeeCount**           | Integer       | Number of employees (default is 1 for each record).                            |\n",
        "| **EmployeeNumber**          | Integer       | Unique identifier for each employee.                                           |\n",
        "| **EnvironmentSatisfaction** | Integer       | Satisfaction with the environment (1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'). |\n",
        "| **Gender**                  | String        | Gender of the employee ('Male' or 'Female').                                   |\n",
        "| **HourlyRate**              | Integer       | Hourly rate of the employee.                                                   |\n",
        "| **JobInvolvement**          | Integer       | Level of job involvement (1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High').   |\n",
        "| **JobLevel**                | Integer       | Job level within the organization.                                             |\n",
        "| **JobRole**                 | String        | Role of the employee within the company.                                       |\n",
        "| **JobSatisfaction**         | Integer       | Job satisfaction level (1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High').     |\n",
        "| **MaritalStatus**           | String        | Marital status of the employee.                                                |\n",
        "| **MonthlyIncome**           | Integer       | Monthly income of the employee.                                                |\n",
        "| **MonthlyRate**             | Integer       | Monthly rate of the employee.                                                  |\n",
        "| **NumCompaniesWorked**      | Integer       | Number of companies the employee has worked for.                               |\n",
        "| **Over18**                  | String        | Whether the employee is over 18 years old ('Yes').                             |\n",
        "| **OverTime**                | String        | Whether the employee works overtime ('Yes' or 'No').                           |\n",
        "| **PercentSalaryHike**       | Integer       | Percentage increase in salary.                                                 |\n",
        "| **PerformanceRating**       | Integer       | Performance rating (1: 'Low', 2: 'Good', 3: 'Excellent', 4: 'Outstanding').    |\n",
        "| **RelationshipSatisfaction**| Integer       | Satisfaction with relationships (1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'). |\n",
        "| **StandardHours**           | Integer       | Standard working hours (default is 80).                                        |\n",
        "| **StockOptionLevel**        | Integer       | Stock option level.                                                            |\n",
        "| **TotalWorkingYears**       | Integer       | Total number of years the employee has worked.                                 |\n",
        "| **TrainingTimesLastYear**   | Integer       | Number of training sessions attended last year.                                |\n",
        "| **WorkLifeBalance**         | Integer       | Work-life balance rating (1: 'Bad', 2: 'Good', 3: 'Better', 4: 'Best').        |\n",
        "| **YearsAtCompany**          | Integer       | Number of years the employee has been with the company.                        |\n",
        "| **YearsInCurrentRole**      | Integer       | Number of years in the current role.                                           |\n",
        "| **YearsSinceLastPromotion** | Integer       | Number of years since the last promotion.                                      |\n",
        "| **YearsWithCurrManager**    | Integer       | Number of years with the current manager.                                      |\n",
        "\n",
        "---\n",
        "\n",
        "**Sources**\n",
        "\n",
        "- Dataset: [Employee Attrition for Healthcare](https://www.kaggle.com/datasets/jpmiller/employee-attrition-for-healthcare)\n",
        "- Data Dictionary: Compiled from the dataset's description and metadata."
      ],
      "metadata": {
        "id": "HrDqRzt-YL06"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYEj5WMNAhLv"
      },
      "source": [
        "### Data Shape\n",
        "\n",
        "Assessing the shape of data helps identify its dimensionality (rows and columns), which is crucial for understanding its structure and determining suitable analysis techniques. It ensures the dataset is in the expected format, enabling error detection and proper preprocessing. Additionally, knowing the data shape aids in resource optimization and selecting the right tools for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oi96O3mNtjE"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import pandas as pd\n",
        "\n",
        "# Check the shape (rows, columns)\n",
        "print('Data set rows and columns:', df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfD_c-zZOI3j"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Print off the first 5 rows in a visually appealing table format\n",
        "print(tabulate(df.head(5), headers='keys', tablefmt='pretty'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5DphqfwN7JT"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import pandas as pd\n",
        "\n",
        "# Lets get more basic information on columns, datatypes etc using .info()\n",
        "print('Feature Information:')\n",
        "print('-' * 40)\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 3: Data Preparation"
      ],
      "metadata": {
        "id": "_DwjAvi6uWHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unnecessary columns\n",
        "\n",
        "Decision trees can grow unnecessarily large as they attempt to evaluate splits based on irrelevant columns.\n",
        "\n",
        "Irrelevant features increase the model's complexity without improving accuracy, making the tree harder to interpret.\n",
        "\n",
        "The model might perform well on training data but poorly on validation or test data."
      ],
      "metadata": {
        "id": "GSUAHFmYua60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Remove Unneccesary Columns"
      ],
      "metadata": {
        "id": "r0AclIovAeB1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b86a5e5"
      },
      "outputs": [],
      "source": [
        "# Columns to drop\n",
        "columns_to_drop = ['EmployeeID',\n",
        "                   'StandardHours',\n",
        "                   'Over18',\n",
        "                   'MonthlyRate',\n",
        "                   'EmployeeCount']\n",
        "\n",
        "# Drop the columns like they are hot\n",
        "try:\n",
        "    df = df.drop(columns=columns_to_drop)\n",
        "    print(f\"Successfully dropped columns: {columns_to_drop}\")\n",
        "except KeyError as e:\n",
        "    print(f\"KeyError encountered: {e}\")\n",
        "    print(\"Please ensure the specified columns exist in the DataFrame.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing Values\n",
        "\n",
        "Decision trees do not inherently handle missing values, and failing to address them can negatively affect the model's performance.\n",
        "\n",
        " Null values can introduce noise, leading to incorrect or suboptimal splits.\n",
        "\n",
        " Splitting decisions may become less reliable or fail entirely if null values are not handled.\n",
        "\n"
      ],
      "metadata": {
        "id": "YszNJ-K-K-d3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Null Values\n",
        "\n",
        "Identify null values, and decide what to do with them."
      ],
      "metadata": {
        "id": "66UioMdUxNJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# NULL Valuse\n",
        "\n",
        "# Function to find columns with NaN values\n",
        "def find_columns_with_nan(df):\n",
        "    columns_with_nan = [col for col in df.columns if df[col].isna().any()]\n",
        "    return columns_with_nan\n",
        "\n",
        "# Identify variables with NaN values\n",
        "columns_with_nan = find_columns_with_nan(df)\n",
        "\n",
        "if columns_with_nan:\n",
        "    print(\"Columns with NaN values:\", columns_with_nan)\n",
        "else:\n",
        "    print(\"There are no NaN values in the dataset.\")\n"
      ],
      "metadata": {
        "id": "sUhlBb1EtxYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Blank Values\n",
        "\n",
        "Indentify blank values and decide what to do with them.\n",
        "\n",
        "In this case, we are going to replace balnk values with the most frequently occurring value (mode)."
      ],
      "metadata": {
        "id": "l51voUPkxRqv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnHtB2z7Ul2n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd  # Importing pandas to handle DataFrames\n",
        "\n",
        "# Missing Values / Blank Values\n",
        "\n",
        "# Identifying missing values in the dataset\n",
        "missing_values = df.isnull().sum().sort_values(ascending=False)\n",
        "missing_values = missing_values[missing_values > 0]  # Filter out columns with no missing values\n",
        "\n",
        "if missing_values.empty:\n",
        "    print(\"There are no missing values in the dataset.\")\n",
        "else:\n",
        "    print(\"Missing values by column:\")\n",
        "    print(missing_values)\n",
        "\n",
        "    # Replacing missing values\n",
        "    for column in df.columns:\n",
        "        if df[column].isnull().any():  # Check if the column has missing values\n",
        "            if df[column].dtype == 'object':  # For object (categorical) columns\n",
        "                mode_value = df[column].mode()[0]\n",
        "                df[column].fillna(mode_value, inplace=True)\n",
        "                print(f\"Missing values in column '{column}' replaced with mode: {mode_value}\")\n",
        "            elif df[column].dtype in ['float64', 'int64']:  # For numeric columns\n",
        "                mean_value = df[column].mean()\n",
        "                df[column].fillna(mean_value, inplace=True)\n",
        "                print(f\"Missing values in column '{column}' replaced with mean: {mean_value:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split **X** Feature Variables from **Y** Target Variable\n",
        "\n",
        "At this point, we are confident that the data in our data frame is clean and tidy. This is the data set that the rest of the model will be based on.\n",
        "\n",
        "We can now go ahead and split the Target Feature away from the data set."
      ],
      "metadata": {
        "id": "2AWXJx08LagJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVF_azAFJz6M"
      },
      "outputs": [],
      "source": [
        "import pandas as pd  # For handling DataFrames\n",
        "\n",
        "# Split data to Target and Feature\n",
        "Target = 'Attrition'\n",
        "\n",
        "# Make a new copy of the columns used to make predictions\n",
        "X = df.drop(Target, axis=1).copy()  # Alternatively: X = df_no_missing.iloc[:, :-1]\n",
        "\n",
        "# Make a new copy of the column of data we want to predict\n",
        "y = df[Target].copy()\n",
        "\n",
        "# Check the shape of X and y\n",
        "print(f\"Shape of X: {X.shape}\")\n",
        "print(f\"Shape of y: {y.shape}\")\n",
        "\n",
        "# If the number of samples is different, raise an error\n",
        "if X.shape[0] != y.shape[0]:\n",
        "    raise ValueError(\"X and y must have the same number of samples.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variable Encoding"
      ],
      "metadata": {
        "id": "XoN59JEoLrYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categorical Variables\n",
        "\n",
        "Machine learning algorithms require numerical inputs to process data. Raw categorical data, like text or labels, cannot be directly used in calculations, and improper encoding might lead to misinterpretation.\n",
        "\n",
        "Encoding methods like one-hot encoding for nominal data or label encoding for ordinal data ensure compatibility with algorithms and improve model performance."
      ],
      "metadata": {
        "id": "mOqVjXI2MvAZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtNRi3dcXKhh"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Identify categorical columns - assuming they are of type 'object'\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "## Print the shape to see how many new columns we have before updating\n",
        "print(f\"Shape before encoding: {X.shape}\")\n",
        "print('-' * 40)\n",
        "\n",
        "# Print unique values for each categorical column before encoding\n",
        "print(\"Unique values in categorical columns before one-hot encoding:\")\n",
        "print('-' * 40)\n",
        "for column in categorical_columns:\n",
        "    print(f\"{column}: {df[column].unique()}\")\n",
        "print('-' * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### One-hot Encoding\n",
        "\n",
        "One-hot encoding is a method to transform categorical data into a numerical format suitable for machine learning algorithms. It converts each category into a binary vector where only one element is \"1\" (indicating the presence of a specific category) and the rest are \"0\".\n",
        "\n",
        "<img src=\"https://github.com/Jrk373/MachineLearningDemo/blob/main/OneHotDiagram.jpg?raw=true\" alt=\"OneHotDiagram\" width=\"500\" height=\"175\" />\n",
        "\n",
        "For example, a feature with categories [\"Apple\", \"Chicken\", \"Broccoli\"] would be transformed into three binary columns: [1, 0, 0] for \"Apple\", [0, 1, 0] for \"Chicken\", and [0, 0, 1] for \"Broccoli\".\n",
        "\n",
        "This technique ensures no ordinal relationship is implied between categories and is widely used for nominal data."
      ],
      "metadata": {
        "id": "-6lu1nCRlXK5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYlrzy5rY_9A"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Apply one-hot encoding to all categorical variables\n",
        "df_encoded = pd.get_dummies(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1tqfLu2lZPFw"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "## Print Results ##\n",
        "## Print the shape to see how many new columns were added\n",
        "print('-' * 40)\n",
        "print(f\"Shape after encoding: {df_encoded.shape}\")\n",
        "print('-' * 40)\n",
        "\n",
        "# Identify new one-hot encoded columns\n",
        "new_columns = [col for col in df_encoded.columns if col not in df.columns]\n",
        "\n",
        "# Print unique values in the new one-hot encoded columns (they should only be 0 or 1)\n",
        "print(\"\\nUnique values in one-hot encoded columns:\")\n",
        "print('-' * 40)\n",
        "for column in new_columns:\n",
        "    print(f\"{column}: {df_encoded[column].unique()}\")\n",
        "print('-' * 40)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  # For handling DataFrames\n",
        "\n",
        "# Count of each data type, including 0 counts\n",
        "data_type_counts = {dtype: (df_encoded.dtypes == dtype).sum() for dtype in [\"bool\", \"category\", \"datetime64[ns]\", \"float64\", \"int64\", \"object\", \"str\"]}\n",
        "\n",
        "# Print the Results\n",
        "print('-' * 40)\n",
        "print(\"Data type counts (including 0 values):\")\n",
        "print('-' * 40)\n",
        "for dtype, count in data_type_counts.items():\n",
        "    print(f\"{dtype}: {count}\")\n",
        "print('-' * 40)"
      ],
      "metadata": {
        "id": "CKTLjV7jxuwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Boolean Variables"
      ],
      "metadata": {
        "id": "jiqaep5AkST3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  # For handling DataFrames\n",
        "\n",
        "# Check for boolean columns and convert them to integers\n",
        "bool_columns = df_encoded.select_dtypes(include=[\"bool\"]).columns\n",
        "for col in bool_columns:\n",
        "    df_encoded[col] = df_encoded[col].astype(int)\n",
        "\n",
        "# Check for object columns in the DataFrame and convert them to integers\n",
        "object_columns = df_encoded.select_dtypes(include=[\"object\"]).columns\n",
        "for col in object_columns:\n",
        "    df_encoded[col] = df_encoded[col].astype(\"category\").cat.codes\n",
        "\n",
        "# Count of each data type, including 0 counts\n",
        "data_type_counts = {dtype: (df_encoded.dtypes == dtype).sum() for dtype in [\"bool\", \"category\", \"datetime64[ns]\", \"float64\", \"int64\", \"object\"]}\n",
        "data_type_counts = {k: v for k, v in sorted(data_type_counts.items())}  # Sort for clarity\n",
        "\n",
        "# Print the Results\n",
        "print(\"-\" * 40)\n",
        "print(\"Data type counts (including 0 values):\")\n",
        "print(\"-\" * 40)\n",
        "for dtype, count in data_type_counts.items():\n",
        "    print(f\"{dtype}: {count}\")\n",
        "print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "IsjwNP3zrU0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Target Variable"
      ],
      "metadata": {
        "id": "rXjIwFK3MzGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  # For handling DataFrames\n",
        "\n",
        "# Print unique values of the target variable before encoding\n",
        "print('Unique y values before encoding:')\n",
        "print(y.unique())"
      ],
      "metadata": {
        "id": "wlLsszOSvfTk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Get unique values\n",
        "unique_values = np.unique(y_encoded)\n",
        "print('Unique y values after encoding:')\n",
        "print(unique_values)"
      ],
      "metadata": {
        "id": "oTpSjGn3fsIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation Summary\n",
        "\n",
        "Imported data, investigated shape and content, cleaned the data, split X and Y variables, encoded data.\n",
        "\n",
        "\n",
        "write to file"
      ],
      "metadata": {
        "id": "Pjd4hgxSp5iF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Visualization\n",
        "\n",
        "Review the data set that we have created by modifying the raw data.\n"
      ],
      "metadata": {
        "id": "nhZamwgmpw9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### X_encoded Data Frame\n",
        "\n",
        "This is the X training data set for the model."
      ],
      "metadata": {
        "id": "7aToopYLqEc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assign df_encoded to X_encoded for clarity\n",
        "X_encoded = df_encoded\n",
        "\n",
        "# Create a DataFrame to store unique values\n",
        "unique_values_df = pd.DataFrame(columns=['Column', 'Unique Values'])\n",
        "\n",
        "# Iterate through columns and add unique values to the DataFrame\n",
        "for col in X_encoded.columns:\n",
        "    unique_values_df = pd.concat([unique_values_df, pd.DataFrame({'Column': [col], 'Unique Values': [X_encoded[col].unique()]})], ignore_index=True)\n",
        "\n",
        "## Print the shape to see how many new columns were added\n",
        "print('-' * 40)\n",
        "print(f\"Shape after encoding: {X_encoded.shape}\")\n",
        "print('-' * 40)\n",
        "\n",
        "# Display the DataFrame with styling\n",
        "unique_values_df.style.set_properties(**{'text-align': 'left'}) \\\n",
        "                      .set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
        "\n"
      ],
      "metadata": {
        "id": "XK2_Fzm6y4ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Continuous Numbers"
      ],
      "metadata": {
        "id": "zWpEiv2BEWsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# List of features to plot\n",
        "features = [\n",
        "    \"HourlyRate\",\n",
        "    \"DailyRate\",\n",
        "    \"MonthlyIncome\",\n",
        "    \"YearsAtCompany\",\n",
        "    \"YearsInCurrentRole\",\n",
        "    \"TotalWorkingYears\",\n",
        "    \"YearsSinceLastPromotion\",\n",
        "    \"YearsWithCurrManager\",\n",
        "    \"PercentSalaryHike\",\n",
        "    'Age',\n",
        "    'DistanceFromHome',\n",
        "    'NumCompaniesWorked'\n",
        "]\n",
        "\n",
        "# Create a grid of plots\n",
        "n_features = len(features)\n",
        "cols = 3\n",
        "rows = (n_features // cols) + (n_features % cols > 0)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(10, 2.5 * rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(features):\n",
        "    sns.histplot(X_encoded[feature], kde=True, ax=axes[i], color=\"#000070\")\n",
        "    axes[i].set_title(f\"Distribution of {feature}\")\n",
        "    axes[i].set_xlabel(feature)\n",
        "    axes[i].set_ylabel(\"Frequency\")\n",
        "\n",
        "# Remove any empty subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Pm5B9CVinGH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Discrete NUmbers"
      ],
      "metadata": {
        "id": "bAAPC6Ogzei-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# List of features to plot\n",
        "features = ['Education',\n",
        "            'JobLevel',\n",
        "            'JobInvolvement',\n",
        "            'TrainingTimesLastYear',\n",
        "            'JobSatisfaction',\n",
        "            'PerformanceRating',\n",
        "            'Shift',\n",
        "            'WorkLifeBalance']\n",
        "\n",
        "# Create a grid of plots\n",
        "n_features = len(features)\n",
        "cols = 3\n",
        "rows = (n_features // cols) + (n_features % cols > 0)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(10, 2.5 * rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(features):\n",
        "    # Use count plots to better display ordinal variables\n",
        "    sns.countplot(x=X_encoded[feature], ax=axes[i], color=\"#000070\")\n",
        "    axes[i].set_title(f\"Count of {feature}\")\n",
        "    axes[i].set_xlabel(feature)\n",
        "    axes[i].set_ylabel(\"Count\")\n",
        "\n",
        "# Remove any empty subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "k4bYAPR4wLuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### One-hot Encoded Variables"
      ],
      "metadata": {
        "id": "h_srF6w7zi-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# List of specific one-hot encoded variables to select\n",
        "variables = [\n",
        "    \"BusinessTravel_Non-Travel\", \"BusinessTravel_Travel_Frequently\", \"BusinessTravel_Travel_Rarely\",\n",
        "    \"Department_Cardiology\", \"Department_Maternity\", \"Department_Neurology\",\n",
        "    \"EducationField_Human Resources\", \"EducationField_Life Sciences\", \"EducationField_Marketing\",\n",
        "    \"EducationField_Medical\", \"EducationField_Other\", \"EducationField_Technical Degree\",\n",
        "    \"Gender_Female\", \"Gender_Male\",\n",
        "    \"JobRole_Admin\", \"JobRole_Administrative\", \"JobRole_Nurse\", \"JobRole_Other\", \"JobRole_Therapist\",\n",
        "    \"MaritalStatus_Divorced\", \"MaritalStatus_Married\", \"MaritalStatus_Single\",\n",
        "    \"OverTime_No\", \"OverTime_Yes\"\n",
        "]\n",
        "\n",
        "# Filter the DataFrame to include only the specified variables\n",
        "X_encoded_OneHot = X_encoded[variables]\n",
        "\n",
        "# Calculate the correlation matrix for one-hot encoded variables\n",
        "correlation_matrix = X_encoded_OneHot.corr()\n",
        "\n",
        "# Create a mask to hide the upper triangle\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "\n",
        "# Plot the heatmap with the mask applied\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    correlation_matrix,\n",
        "    mask=mask,\n",
        "    annot=False,\n",
        "    cmap=\"coolwarm\",\n",
        "    linewidths=0.5,\n",
        "    vmin=-1,  # Set the minimum value of the heatmap\n",
        "    vmax=1    # Set the maximum value of the heatmap\n",
        ")\n",
        "plt.title(\"Heatmap of One-Hot Encoded Variables Correlation (Lower Triangle)\")\n",
        "plt.xlabel(\"Variables\")\n",
        "plt.ylabel(\"Variables\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "x3UMovIlIDsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 5  # Number of top correlations to show\n",
        "variable = \"Department_Cardiology\"  # Example variable to focus on\n",
        "\n",
        "# Get top correlations for the selected variable\n",
        "top_correlations = correlation_matrix[variable].sort_values(key=abs, ascending=False)[1:top_n + 1]\n",
        "\n",
        "# Plot the bar chart\n",
        "top_correlations.plot(kind=\"bar\", color=\"skyblue\", figsize=(8, 6))\n",
        "plt.title(f\"Top {top_n} Correlations for {variable}\")\n",
        "plt.ylabel(\"Correlation\")\n",
        "plt.xlabel(\"Variables\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9m9DH5CJjJXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Y_encoded Data Frame\n",
        "\n",
        "This is the Y training data set for the model."
      ],
      "metadata": {
        "id": "GOJXlq6YK6UK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming y_train contains the target variable (Attrition) for the training data\n",
        "\n",
        "# Create a countplot\n",
        "sns.countplot(x=y_encoded)\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel(\"Attrition\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of Attrition in Training Data\")\n",
        "\n",
        "# Customize plot appearance (optional)\n",
        "plt.xticks([0, 1], ['No', 'Yes'])  # Replace 0 and 1 with your actual labels if different\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EiqHgRykqJDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 4: Modeling\n",
        "\n",
        "The modeling phase of CRISP-DM (Cross-Industry Standard Process for Data Mining) involves selecting and applying appropriate machine learning or statistical models to address the business problem.\n",
        "\n",
        "It includes tasks such as choosing suitable algorithms, tuning model parameters, and training models on the prepared dataset. This phase also involves evaluating multiple models to identify the one that performs best based on predefined metrics, ensuring it aligns with the project's goals.\n",
        "\n",
        "The results from this phase provide insights into the data and form the basis for deploying a solution.\n",
        "\n",
        "The first parts of this process will be determineng correlations and importnace. We may need to return to shaping to scale variables if needed."
      ],
      "metadata": {
        "id": "v5VumTKWNEn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Test Split\n",
        "\n",
        "Train-test split is a common method used in machine learning to evaluate the performance of a model. It involves dividing a dataset into two subsets: one for training the model (the training set) and one for testing its performance (the test set). This ensures that the model is evaluated on data it has not seen before, providing an unbiased assessment of its ability to generalize to new data."
      ],
      "metadata": {
        "id": "9CF6fj_fNIrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  # For handling DataFrames\n",
        "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
        "\n",
        "# Splitting data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=373)\n",
        "\n",
        "# Confirming the split was successful\n",
        "if len(X_train) + len(X_test) == len(X_encoded) and len(y_train) + len(y_test) == len(y_encoded):\n",
        "    print(\"Data split successful!\")\n",
        "    print(f\"Training set size: {len(X_train)} samples\")\n",
        "    print(f\"Testing set size: {len(X_test)} samples\")\n",
        "else:\n",
        "    print(\"Data split unsuccessful. Please check your data!\")\n"
      ],
      "metadata": {
        "id": "7Y0axODBlBrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Classifier"
      ],
      "metadata": {
        "id": "vZVsrEHlNXOs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36bbbefa"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "NameOfModel = 'clf_dt'\n",
        "\n",
        "## create a decisiont tree and fit it to the training data\n",
        "clf_dt = DecisionTreeClassifier(random_state=373)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model"
      ],
      "metadata": {
        "id": "_c2zkjY_OBRd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ba1a565"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Fit (train) the model\n",
        "clf_dt.fit(X = X_train, y = y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Decision Tree Plot\n",
        "\n",
        "**A Note on overfitting**\n",
        "\n",
        "Overfitting a decision tree classifier occurs when the model becomes too complex, capturing noise and specific patterns in the training data rather than generalizable trends. This results in excellent performance on the training dataset but poor performance on unseen data, as the model fails to generalize. Overfitting can lead to overly deep trees with many splits, creating rules that are too specific to the training data. This risk can be mitigated by techniques like pruning the tree, setting a maximum depth, limiting the minimum number of samples per leaf, or using cross-validation to balance model complexity and accuracy."
      ],
      "metadata": {
        "id": "pEeK_LxuTAtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt  # For plotting\n",
        "from sklearn.tree import plot_tree  # For visualizing decision trees\n",
        "\n",
        "# NOTE: We can plot the tree and it is huge!\n",
        "plt.figure(figsize=(12, 4))\n",
        "plot_tree(clf_dt,\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          class_names=[\"No Attrition\", \"Yes Attrition\"],\n",
        "          feature_names=X_encoded.columns)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VwYOtr7L2G0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Confusion Matrix\n",
        "\n",
        "A confusion matrix is a table used to evaluate the performance of a classification model by comparing its predictions to the actual outcomes. It shows the counts of true positives, true negatives, false positives, and false negatives, providing insights into the model's accuracy, precision, recall, and other metrics."
      ],
      "metadata": {
        "id": "ocwGNgmhTGAK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkaBJr2DJz6Q"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "\n",
        "# Generate predictions using your trained model (assuming clf_dt is already defined)\n",
        "y_pred = clf_dt.predict(X_test)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a figure for plotting\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Plotting the Confusion Matrix with percentage annotations\n",
        "cmd = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                             display_labels=[\"No Attrition\", \"Yes Attrition\"])\n",
        "cmd.plot(cmap=plt.cm.Blues, colorbar=True, ax=ax)\n",
        "\n",
        "# Add percentage labels on top of each cell in the confusion matrix\n",
        "total = cm.sum()\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        if total > 0:\n",
        "            percent_text = f\"{(cm[i, j] / total) * 100:.2f}%\"\n",
        "        else:\n",
        "            percent_text = \"0.00%\"\n",
        "\n",
        "        # Positioning text at center of each cell with proper formatting and adjusted positions\n",
        "        ax.text(j, i - 0.1, percent_text,  # Adjust vertical position here (i - 0.1)\n",
        "                ha='center', va='center', color='white' if cm[i, j] > total / 2 else 'black',\n",
        "                fontsize=10)  # Adjust font size if needed\n",
        "\n",
        "plt.title(\"Confusion Matrix: Decision Tree Classifier\")\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from being cut off\n",
        "\n",
        "# Save the plot directly using savefig\n",
        "plt.savefig(\"CrossVal_Orig.png\")  # Save the plot as an image\n",
        "plt.show()\n",
        "\n",
        "# Inform user about the saved plot\n",
        "print(\"Plot saved as 'CrossVal_Orig.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Cross Validation"
      ],
      "metadata": {
        "id": "Hm4P42Djpmpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "# Cross Validation\n",
        "model = clf_dt  # Assuming 'clf_dt' is your trained DecisionTreeClassifier\n",
        "X = X_test     # Assuming 'X_test' is your feature data for testing\n",
        "y = y_test     # Assuming 'y_test' is your target variable data for testing\n",
        "cv_folds = 5   # Number of cross-validation folds\n",
        "\n",
        "\n",
        "# --- Cross-validation Scores ---\n",
        "cv_scores = cross_val_score(model, X, y, cv=cv_folds)\n",
        "\n",
        "print(\"\\nCross-validation accuracy provides an estimate of how well the model generalizes to unseen data by evaluating its performance on different subsets of the data.\")\n",
        "print('-' * 40)\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean CV accuracy:\", cv_scores.mean())\n",
        "print('-' * 40)\n",
        "\n",
        "# --- Cross-validation Predictions ---\n",
        "y_pred = cross_val_predict(model, X, y, cv=cv_folds)\n",
        "\n",
        "\n",
        "# --- Performance Metrics ---\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "precision = precision_score(y, y_pred)\n",
        "recall = recall_score(y, y_pred)\n",
        "f1 = f1_score(y, y_pred)\n",
        "\n",
        "# --- Confusion Matrix ---\n",
        "cm = confusion_matrix(y, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "print('-' * 40)\n",
        "\n",
        "\n",
        "\n",
        "# --- Classification Report ---\n",
        "print(\"\\nThe classification report provides a comprehensive evaluation of the model's performance for each class, including precision, recall, F1-score, and support.\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y, y_pred))\n",
        "print('-' * 40)\n",
        "\n",
        "\n",
        "# --- Visualizing Cross-validation Scores ---\n",
        "print(\"\\nThis plot visualizes the accuracy scores for each fold of cross-validation, providing insights into the model's consistency and stability across different data subsets.\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, cv_folds + 1), cv_scores, marker='o', linestyle='--')\n",
        "plt.xlabel(\"Fold\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Cross-validation Scores per Fold\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "WMpbGOh5SWNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Importance Matrix\n",
        "\n",
        "Gini Importance (or Mean Decrease in Impurity):\n",
        "\n",
        "Decision Trees use a metric called Gini Impurity to measure the disorder or uncertainty within a set of data.\n",
        "\n",
        "When a tree is built, it aims to make splits that minimize this Gini Impurity, effectively creating more homogeneous subsets of data.\n",
        "\n",
        "Feature importance is calculated by measuring how much each feature contributes to reducing the overall Gini Impurity of the tree."
      ],
      "metadata": {
        "id": "R221Eo5JpsM2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d953fb55"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get feature importances from the trained model\n",
        "importances = clf_dt.feature_importances_\n",
        "\n",
        "# Create a DataFrame to store feature importances\n",
        "important_features = pd.DataFrame({\n",
        "    'feature': X_train.columns,  # Assuming X_train contains your feature names\n",
        "    'importance': importances\n",
        "}).sort_values('importance', ascending=True)  # Sort in descending order\n",
        "\n",
        "# Display the important features\n",
        "#print(\"Important Features:\")\n",
        "#print(important_features)\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(5, 6))  # Adjust figure size if needed\n",
        "plt.barh(important_features['feature'], important_features['importance'])\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance')\n",
        "plt.yticks(fontsize=5)  # Adjust the font size as needed\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Feature Selection\n",
        "\n",
        "Benefits of Removing Low-Importance Variables:\n",
        "\n",
        "- **Improved Model Simplicity and Interpretability:** Removing irrelevant or redundant features makes the decision tree easier to understand and visualize.\n",
        "- **Reduced Overfitting:** By focusing on the most important features, you can reduce the risk of the model overfitting to noise in the training data, leading to better generalization to unseen data.\n",
        "- **Faster Training:** With fewer features to consider, the model can be trained more quickly.\n",
        "- **Potential Performance Improvement:** In some cases, removing noisy or irrelevant features can slightly improve the model's predictive accuracy.\n"
      ],
      "metadata": {
        "id": "hwarXoX6WUQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Get feature importances from the trained model\n",
        "importances = clf_dt.feature_importances_\n",
        "\n",
        "# Create a DataFrame to store feature importances\n",
        "important_features = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': importances\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Filter out features with importance <= 0\n",
        "filtered_features = important_features[important_features['importance'] > 0]['feature']\n",
        "\n",
        "# *** Confirmation Method ***\n",
        "# Create a Markdown string for the selected features\n",
        "selected_features_md = \"**Selected Features:**\\n\\n\"\n",
        "for feature in filtered_features.tolist():\n",
        "    selected_features_md += f\"- {feature}\\n\"\n",
        "\n",
        "# Display the Markdown string using IPython.display.Markdown\n",
        "display(Markdown(selected_features_md))\n",
        "\n",
        "# Optionally, you can also print the number of selected features:\n",
        "print(f\"\\nNumber of selected features: {len(filtered_features)}\")\n",
        "\n",
        "# Check if any important features were accidentally removed\n",
        "if any(important_features[important_features['importance'] > 0.05]['feature'].isin(filtered_features) == False) :\n",
        "  raise ValueError(\"Important Features Removed!!\")\n",
        "\n",
        "# Update X_train and X_test with selected features\n",
        "X_train_filtered = X_train[filtered_features]\n",
        "X_test_filtered = X_test[filtered_features]"
      ],
      "metadata": {
        "id": "po4l2f8Ghz1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Re-iterate modeling and assessment"
      ],
      "metadata": {
        "id": "MEYfvLOBXCUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Filtered Decision Tree Classifier"
      ],
      "metadata": {
        "id": "-hoIV1oil5H6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT1RrSdzl5H8"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "## create a decisiont tree and fit it to the training data\n",
        "clf_dt_filtered = DecisionTreeClassifier(random_state=373)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Fit Filtered Model"
      ],
      "metadata": {
        "id": "KXq6g7YTl5H8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF_7x86yl5H9"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Fit (train) the model\n",
        "clf_dt_filtered.fit(X = X_train_filtered, y = y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Filtered Cross Validation"
      ],
      "metadata": {
        "id": "bXUPu_1unIYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "\n",
        "# Assuming 'clf_dt_filtered' is a trained DecisionTreeClassifier\n",
        "# Assuming 'X_test_filtered' is the feature test data and 'y_test' is the target test data\n",
        "\n",
        "# Parameters\n",
        "cv_folds = 5  # Number of cross-validation folds\n",
        "\n",
        "# --- Cross-validation Scores ---\n",
        "cv_scores = cross_val_score(clf_dt_filtered, X_test_filtered, y_test, cv=cv_folds)\n",
        "\n",
        "print(\"\\nCross-validation accuracy provides an estimate of how well the model generalizes to unseen data.\")\n",
        "print('-' * 40)\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean CV accuracy:\", cv_scores.mean())\n",
        "print('-' * 40)\n",
        "\n",
        "# --- Cross-validation Predictions ---\n",
        "y_pred_cv = cross_val_predict(clf_dt_filtered, X_test_filtered, y_test, cv=cv_folds)\n",
        "\n",
        "# --- Performance Metrics ---\n",
        "accuracy = accuracy_score(y_test, y_pred_cv)\n",
        "precision = precision_score(y_test, y_pred_cv, average='binary')\n",
        "recall = recall_score(y_test, y_pred_cv, average='binary')\n",
        "f1 = f1_score(y_test, y_pred_cv, average='binary')\n",
        "\n",
        "# --- Confusion Matrix ---\n",
        "# Generate predictions for confusion matrix\n",
        "y_pred_cm = clf_dt_filtered.predict(X_test_filtered)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_cm)\n",
        "\n",
        "# --- Classification Report ---\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_cm))\n",
        "print('-' * 40)\n",
        "\n",
        "# --- Visualizing Cross-validation Scores ---\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(range(1, cv_folds + 1), cv_scores, marker='o', linestyle='--')\n",
        "plt.xlabel(\"Fold\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Cross-validation Scores per Fold\")\n",
        "plt.show()\n",
        "\n",
        "# Create a figure for plotting\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "\n",
        "# Plotting the Confusion Matrix with percentage annotations\n",
        "cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Attrition\", \"Yes Attrition\"])\n",
        "cmd.plot(cmap=plt.cm.Blues, colorbar=True, ax=ax)\n",
        "\n",
        "# Add percentage labels below each value in the confusion matrix\n",
        "total = cm.sum()\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        value_text = f\"{cm[i, j]}\"\n",
        "        percent_text = f\"{(cm[i, j] / total) * 100:.2f}%\" if total > 0 else \"0.00%\"\n",
        "        ax.text(j, i, value_text, ha='center', va='center', color='white' if cm[i, j] > total / 2 else 'black')\n",
        "        ax.text(j, i + 0.2, percent_text, ha='center', va='center', fontsize=9, color='black')\n",
        "\n",
        "plt.title(\"Confusion Matrix: Decision Tree Classifier\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the plot directly using savefig\n",
        "plt.savefig(\"CrossVal_Featured.png\")  # Save the plot as an image\n",
        "plt.show()\n",
        "\n",
        "# Inform user about the saved plot\n",
        "print(\"Plot saved as 'CrossVal_Featured.png'\")"
      ],
      "metadata": {
        "id": "rY6OhSw4nIYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyper-parameter Tuning"
      ],
      "metadata": {
        "id": "urWcj1HbOGZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Gridsearch\n",
        "\n",
        "GridSearch is a technique in machine learning and statistical modeling used to tune hyperparameters of a model. It systematically evaluates a range of hyperparameter combinations to identify the best set of parameters that optimize a model's performance. This is particularly useful for improving the accuracy and generalization of predictive models."
      ],
      "metadata": {
        "id": "k2sfR9mtFaKj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3502bfe"
      },
      "outputs": [],
      "source": [
        "# Initial Parameter grid tuning, default settings\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],  # The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
        "    #'splitter': ['best', 'random'],  # The strategy used to choose the split at each node. Supported strategies are \"best\" to choose the best split and \"random\" to choose the best random split.\n",
        "    'max_depth': [None, 5, 10, 15, 20, 30, 40, 50],  # The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
        "    'min_samples_split': [2, 5, 10, 20, 50, 100],  # The minimum number of samples required to split an internal node.\n",
        "    #'min_samples_leaf': [1, 2, 4, 8, 20, 50],  # The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
        "    #'min_weight_fraction_leaf': [0.0, 0.01, 0.05, 0.1],  # The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
        "    #'max_features': ['auto', 'sqrt', 'log2', None],  # The number of features to consider when looking for the best split:\n",
        "                                                    # - 'auto': This is equivalent to 'sqrt' (see below).\n",
        "                                                    # - 'sqrt': This will use the square root of the total number of features at each split. For example, if you have 16 features, it will consider 4 features at each split (sqrt(16) = 4).\n",
        "                                                    # - 'log2': This will use the base-2 logarithm of the total number of features at each split. For example, if you have 16 features, it will consider 4 features at each split (log2(16) = 4).\n",
        "    #'max_leaf_nodes': [None, 10, 20, 50, 100],  # Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
        "    'min_impurity_decrease': [0.0, 0.001, 0.01, 0.1]#,  # A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
        "    #'class_weight': [None, 'balanced'],  # Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n",
        "                                          # - The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n",
        "                                          # - Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.\n",
        "    #'ccp_alpha': [0.0, 0.01, 0.1, 1.0],  # Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See Minimal Cost-Complexity Pruning for details.\n",
        "}\n",
        "\n",
        "cv_folds = 5  # Number of folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eeec1aa"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# GridSearchCV\n",
        "gridSearch = GridSearchCV(\n",
        "    estimator=clf_dt_filtered,         # Model to optimize\n",
        "    param_grid=param_grid,    # Parameter grid\n",
        "    cv=cv_folds,                     # Number of cross-validation folds\n",
        "    scoring='accuracy',       # Scoring metric (can be adjusted)\n",
        "    verbose=1                 # Print progress during search\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "gridSearch.fit(X_train_filtered, y_train)\n",
        "\n",
        "# Print best parameters and best score\n",
        "print('Best score: ', gridSearch.best_score_)\n",
        "print('Best parameters: ', gridSearch.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the best decision tree model from GridSearchCV\n",
        "best_clf = gridSearch.best_estimator_\n",
        "\n",
        "# Plot the decision tree\n",
        "plt.figure(figsize=(20, 10))  # Adjust the size for better visibility\n",
        "plot_tree(\n",
        "    best_clf,\n",
        "    feature_names=X_train_filtered.columns,  # Replace with actual feature names\n",
        "    class_names=[\"No Attrition\", \"Yes Attrition\"],  # Replace with actual class names\n",
        "    filled=True,                     # Color the nodes based on class distribution\n",
        "    rounded=True                     # Rounded corners for better readability\n",
        ")\n",
        "\n",
        "plt.title(\"Initial Decision Tree Visualization\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3c08WbqeXq1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Cross Validation\n",
        "\n",
        "Cross-validation is a statistical technique used in machine learning to assess how well a model generalizes to unseen data. It divides the dataset into multiple subsets (or folds) and iteratively trains and evaluates the model on different splits of the data. This ensures that the model is tested on data it hasn't seen during training, which provides a robust estimate of its performance."
      ],
      "metadata": {
        "id": "2FwlHsrLtfZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "# Cross Validation\n",
        "model = clf_dt_filtered  # Assuming 'clf_dt' is your trained DecisionTreeClassifier\n",
        "X = X_test_filtered     # Assuming 'X_test' is your feature data for testing\n",
        "y = y_test     # Assuming 'y_test' is your target variable data for testing\n",
        "cv_folds = 5   # Number of cross-validation folds\n",
        "\n",
        "\n",
        "# --- Cross-validation Scores ---\n",
        "cv_scores = cross_val_score(model, X, y, cv=cv_folds)\n",
        "\n",
        "print(\"\\nCross-validation accuracy provides an estimate of how well the model generalizes to unseen data by evaluating its performance on different subsets of the data.\")\n",
        "print('-' * 40)\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean CV accuracy:\", cv_scores.mean())\n",
        "print('-' * 40)\n",
        "\n",
        "# --- Cross-validation Predictions ---\n",
        "y_pred = cross_val_predict(model, X, y, cv=cv_folds)\n",
        "\n",
        "\n",
        "# --- Performance Metrics ---\n",
        "#accuracy = accuracy_score(y, y_pred)\n",
        "#precision = precision_score(y, y_pred)\n",
        "#recall = recall_score(y, y_pred)\n",
        "#f1 = f1_score(y, y_pred)\n",
        "\n",
        "#print(\"\\nPerformance Metrics:\")\n",
        "#print('-' * 40)\n",
        "#print(f\"Accuracy: {accuracy:.4f}\")\n",
        "#print(f\"Precision: {precision:.4f}\")\n",
        "#print(f\"Recall: {recall:.4f}\")\n",
        "#print(f\"F1-score: {f1:.4f}\")\n",
        "#print('-' * 40)\n",
        "#print(\"\\nAccuracy: Measures the overall correctness of the model's predictions.\")\n",
        "#print(\"Precision: Out of all the positive predictions, how many were actually positive.\")\n",
        "#print(\"Recall: Out of all the actual positive cases, how many did the model correctly identify.\")\n",
        "#print(\"F1-score: A balanced measure considering both precision and recall.\")\n",
        "#print('-' * 40)\n",
        "\n",
        "# --- Confusion Matrix ---\n",
        "cm = confusion_matrix(y, y_pred)\n",
        "\n",
        "print(\"\\nThe confusion matrix provides a detailed breakdown of the model's predictions, showing the counts of true positives, true negatives, false positives, and false negatives.\")\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "print('-' * 40)\n",
        "\n",
        "# --- Visualizing Cross-validation Scores ---\n",
        "print(\"\\nThis plot visualizes the accuracy scores for each fold of cross-validation, providing insights into the model's consistency and stability across different data subsets.\")\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(range(1, cv_folds + 1), cv_scores, marker='o', linestyle='--')\n",
        "plt.xlabel(\"Fold\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Cross-validation Scores per Fold\")\n",
        "plt.show()\n",
        "print('-' * 40)\n",
        "\n",
        "# --- Classification Report ---\n",
        "print(\"\\nThe classification report provides a comprehensive evaluation of the model's performance for each class, including precision, recall, F1-score, and support.\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y, y_pred))\n",
        "print('-' * 40)\n",
        "print(\"\\nAccuracy: Measures the overall correctness of the model's predictions.\")\n",
        "print(\"Precision: Out of all the positive predictions, how many were actually positive.\")\n",
        "print(\"Recall: Out of all the actual positive cases, how many did the model correctly identify.\")\n",
        "print(\"F1-score: A balanced measure considering both precision and recall.\")\n",
        "print('-' * 40)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IC-Y8p-LtfZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Tune"
      ],
      "metadata": {
        "id": "yr2lXEJ_AXhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-Print best parameters and best score\n",
        "print('Original Model - Best Results')\n",
        "print('-' * 40)\n",
        "print('Best score: ', gridSearch.best_score_)\n",
        "print('Best parameters: ', gridSearch.best_params_)"
      ],
      "metadata": {
        "id": "obmuOf7qJf-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec0f3c25"
      },
      "outputs": [],
      "source": [
        "# Initial Parameter grid tuning\n",
        "# For reference, don't change\n",
        "\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 5, 10, 15, 20, 30, 40, 50],\n",
        "    'min_impurity_decrease': [0.0, 0.001, 0.01, 0.1],\n",
        "    'min_samples_split': [2, 5, 10, 20, 50, 100],\n",
        "}\n",
        "\n",
        "cv_folds = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0677af90"
      },
      "outputs": [],
      "source": [
        "# Updated param_grid\n",
        "param_grid = {\n",
        "        'criterion': ['entropy'],\n",
        "        'max_depth': [5],\n",
        "        'min_impurity_decrease': [0.0001],\n",
        "        'min_samples_split': [100]  # Updated values\n",
        "    }\n",
        "\n",
        "# Re-run GridSearchCV\n",
        "gridSearch = GridSearchCV(clf_dt_filtered, param_grid, cv=cv_folds)\n",
        "gridSearch.fit(X_train_filtered, y_train)\n",
        "\n",
        "# Print best parameters and best score\n",
        "print('Best score: ', gridSearch.best_score_)\n",
        "print('Best parameters: ', gridSearch.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8634382c"
      },
      "outputs": [],
      "source": [
        "# Evaluate the final model on validatoipn data\n",
        "best_clf = gridSearch.best_estimator_\n",
        "y_pred = best_clf.predict(X_test_filtered)\n",
        "valid_accuracy = best_clf.score(X_test_filtered, y_test)\n",
        "\n",
        "print(\"Accuracy on validation set:\", valid_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 5: Evaluation"
      ],
      "metadata": {
        "id": "5suPgn8ReqOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Model"
      ],
      "metadata": {
        "id": "0CwhlgcWRHB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Final Decision Tree Plot"
      ],
      "metadata": {
        "id": "wvpNPSmsTSQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import plot_tree\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the decision tree\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(\n",
        "    best_clf,\n",
        "    feature_names=X_train_filtered.columns,  # Replace with your actual feature names\n",
        "    class_names=best_clf.classes_.astype(str),  # Replace with your class names if available\n",
        "    filled=True,  # Color the nodes\n",
        "    rounded=True  # Rounded corners for readability\n",
        ")\n",
        "plt.title(\"Decision Tree Visualization\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UHr7mBpBRi1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Final Confusion Matrix"
      ],
      "metadata": {
        "id": "TFzGo3yCTX5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "normalized_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # Calculate percentages\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_clf.classes_)\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False)\n",
        "\n",
        "# Annotate percentages below the values\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i + 0.2, f\"{normalized_cm[i, j]:.1f}%\", ha=\"center\", va=\"center\", fontsize=10, color=\"black\")\n",
        "\n",
        "plt.title(\"Confusion Matrix with Percentages\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DktZfKpXSEcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Final Cross Validation Report"
      ],
      "metadata": {
        "id": "cxk_34lBcrKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Classification Report ---\n",
        "print(\"\\nThe classification report provides a comprehensive evaluation of the model's performance for each class, including precision, recall, F1-score, and support.\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y, y_pred))\n",
        "print('-' * 40)\n",
        "print(\"\\nAccuracy: Measures the overall correctness of the model's predictions.\")\n",
        "print(\"Precision: Out of all the positive predictions, how many were actually positive.\")\n",
        "print(\"Recall: Out of all the actual positive cases, how many did the model correctly identify.\")\n",
        "print(\"F1-score: A balanced measure considering both precision and recall.\")\n",
        "print('-' * 40)\n"
      ],
      "metadata": {
        "id": "8NTacp-0cqQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 6: Deploy"
      ],
      "metadata": {
        "id": "qu0d1cXPdrPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So that's it!\n",
        "\n",
        "Let's review what we have done:\n",
        "\n",
        " 1. Collected some data\n",
        " 2. Reviewed, cleaned, shaped, and transformed the data\n",
        " 3. Built the Decision Classifier Tree model\n",
        " 4. Tested the model on increasingly fine tuned parameters\n",
        " 5. Produced the final model!\n",
        "\n",
        "Now we will deploy this model to a web app"
      ],
      "metadata": {
        "id": "6yUHDBeZe-8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "g-ms3qHjiikn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eiaNee7MeuuU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}